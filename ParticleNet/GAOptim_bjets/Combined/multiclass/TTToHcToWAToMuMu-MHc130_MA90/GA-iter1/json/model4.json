{
  "hyperparameters": {
    "signal": "MHc130_MA90",
    "channel": "Combined",
    "iteration": 1,
    "model_idx": 4,
    "num_hidden": 96,
    "optimizer": "Adadelta",
    "initial_lr": 0.0023,
    "weight_decay": 2e-05,
    "scheduler": "ReduceLROnPlateau",
    "pilot_mode": false,
    "num_classes": 4,
    "model_type": "ParticleNet",
    "num_node_features": 9,
    "num_graph_features": 4,
    "dropout_p": 0.4,
    "batch_size": 1024,
    "train_folds": [
      0,
      1,
      2
    ],
    "valid_folds": [
      3
    ]
  },
  "training_summary": {
    "best_epoch": 1,
    "best_train_loss": 1.3567842315499488,
    "best_valid_loss": 1.37606896589142,
    "best_train_acc": 0.28913226037581763,
    "best_valid_acc": 0.2805204852779584,
    "total_epochs": 9
  },
  "epoch_history": {
    "train_loss": [
      1.3630130983976296,
      1.3567842315499488,
      1.3665475477708275,
      1.3885743988854613,
      1.4224179340982877,
      1.4638345679288116,
      1.4669820627890706,
      1.445605799249138,
      1.4084353814515613
    ],
    "valid_loss": [
      1.3811467626048677,
      1.37606896589142,
      1.3866458961514705,
      1.410740902549342,
      1.4483094918891746,
      1.491463594960821,
      1.4998254746701898,
      1.4807202026484898,
      1.446304436774482
    ],
    "train_acc": [
      0.2927629298057736,
      0.28913226037581763,
      0.2889531345100426,
      0.28895506059462084,
      0.289203525505212,
      0.29091966686441134,
      0.3021025139255915,
      0.32385763923665417,
      0.3623196221792491
    ],
    "valid_acc": [
      0.28418698001928555,
      0.2805204852779584,
      0.2803635099679322,
      0.28034108492364274,
      0.28064382302155044,
      0.28248828291435873,
      0.2940876370730832,
      0.31526809140448053,
      0.35320566008117865
    ],
    "epoch": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ],
    "timestamp": [
      1762540619.5132828,
      1762541002.0971093,
      1762541385.2397456,
      1762541767.783158,
      1762542150.0178518,
      1762542533.0429657,
      1762542915.4976327,
      1762543298.677342,
      1762543681.2832658
    ]
  }
}