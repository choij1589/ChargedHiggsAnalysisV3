{
  "hyperparameters": {
    "signal": "MHc115_MA87",
    "channel": "Run3Mu",
    "iteration": 2,
    "model_idx": 17,
    "num_hidden": 96,
    "optimizer": "RMSprop",
    "initial_lr": 0.0005,
    "weight_decay": 0.00022,
    "scheduler": "ExponentialLR",
    "pilot_mode": false,
    "num_classes": 4,
    "model_type": "ParticleNet",
    "num_node_features": 9,
    "num_graph_features": 4,
    "dropout_p": 0.3,
    "batch_size": 1024,
    "train_folds": [
      0,
      1,
      2
    ],
    "valid_folds": [
      3
    ]
  },
  "training_summary": {
    "best_epoch": 15,
    "best_train_loss": 0.7603996779186388,
    "best_valid_loss": 0.7740868750370464,
    "best_train_acc": 0.6994301606347854,
    "best_valid_acc": 0.6949873142736712,
    "total_epochs": 23
  },
  "epoch_history": {
    "train_loss": [
      0.7959472011099624,
      0.7797489474051886,
      0.7737279438046966,
      0.7718483182798872,
      0.7766422982275168,
      0.7816593581800164,
      0.7809720027672309,
      0.767669887422914,
      0.7720835058545095,
      0.7678396004479336,
      0.7705354780402124,
      0.7644620122153144,
      0.7705711349186901,
      0.7696122471238159,
      0.7716294633841856,
      0.7603996779186388,
      0.771297106463293,
      0.7703961608576073,
      0.7710141308266462,
      0.7645657453164187,
      0.7790272520107508,
      0.7622368017424259,
      0.7887682144313982
    ],
    "valid_loss": [
      0.8096068362774885,
      0.7933221621422418,
      0.7875307863233881,
      0.7859270333812806,
      0.789881509352636,
      0.7955896389514651,
      0.7944894296353183,
      0.7812526275885354,
      0.7858237096835401,
      0.7817091398039615,
      0.7843968427758994,
      0.7776308267871631,
      0.7846921340549184,
      0.782341797421198,
      0.7848149976903739,
      0.7740868750370464,
      0.7850423931898668,
      0.7841417952589662,
      0.7851069529065999,
      0.7785284560031311,
      0.7923435607997726,
      0.7762846326855446,
      0.8023309288422863
    ],
    "train_acc": [
      0.6936805022660607,
      0.6959681902433632,
      0.695498797682194,
      0.696032271139086,
      0.6992154896341141,
      0.6959329457507157,
      0.6962293198934335,
      0.6974548670241313,
      0.6986868222444014,
      0.6993276312016289,
      0.7006444936087317,
      0.6994573950154676,
      0.6973731638820847,
      0.7010578153861434,
      0.6971504827694481,
      0.6994301606347854,
      0.6966955084098165,
      0.698609925169534,
      0.6947426431126654,
      0.7013301591929652,
      0.7004955055261762,
      0.6995214759111903,
      0.6911509089074047
    ],
    "valid_acc": [
      0.6892726580041665,
      0.6919476034920198,
      0.6910559549960686,
      0.6913923496559048,
      0.6943874780127587,
      0.6910235314143977,
      0.6907925133949921,
      0.6927257694521225,
      0.6941199834639733,
      0.694626601927582,
      0.6951007968095195,
      0.6945252782348602,
      0.6929811051577812,
      0.6961505102661165,
      0.6931107994844651,
      0.6949873142736712,
      0.6925109632235525,
      0.6938241182812259,
      0.6899778709055095,
      0.6962194103771673,
      0.695303444194963,
      0.6947968257313544,
      0.6875217845939352
    ],
    "epoch": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8,
      9,
      10,
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      19,
      20,
      21,
      22
    ],
    "timestamp": [
      1761079264.126944,
      1761079627.6133063,
      1761079938.662813,
      1761080279.7244513,
      1761080641.070414,
      1761081006.4722505,
      1761081316.5153506,
      1761081673.3897297,
      1761082043.9456758,
      1761082400.0154958,
      1761082664.047212,
      1761082906.6572614,
      1761083228.3926632,
      1761083552.8240075,
      1761083879.9504716,
      1761084204.2331934,
      1761084890.462256,
      1761085201.0901349,
      1761085491.6234531,
      1761085801.2277255,
      1761086088.7361841,
      1761086393.4208102,
      1761086690.803019
    ]
  }
}