{
  "hyperparameters": {
    "signal": "MHc130_MA90",
    "channel": "Run3Mu",
    "iteration": 1,
    "model_idx": 9,
    "num_hidden": 128,
    "optimizer": "Adadelta",
    "initial_lr": 0.0003,
    "weight_decay": 1e-05,
    "scheduler": "ReduceLROnPlateau",
    "pilot_mode": false,
    "num_classes": 4,
    "model_type": "ParticleNet",
    "num_node_features": 9,
    "num_graph_features": 4,
    "dropout_p": 0.4,
    "batch_size": 1024,
    "train_folds": [
      0,
      1,
      2
    ],
    "valid_folds": [
      3
    ]
  },
  "training_summary": {
    "best_epoch": 1,
    "best_train_loss": 1.354885432096222,
    "best_valid_loss": 1.3807705239164578,
    "best_train_acc": 0.4073773833343486,
    "best_valid_acc": 0.38937983272842036,
    "total_epochs": 9
  },
  "epoch_history": {
    "train_loss": [
      1.3555099475650152,
      1.354885432096222,
      1.3576863928998462,
      1.3600337197409267,
      1.3670606720632938,
      1.3751815902362254,
      1.3876219549969557,
      1.4023576919696306,
      1.4162045291360648
    ],
    "valid_loss": [
      1.3812904010772906,
      1.3807705239164578,
      1.3837710294221366,
      1.3858191448185786,
      1.392771089969294,
      1.4006895647514976,
      1.4130180619339985,
      1.427414521132117,
      1.441359832733172
    ],
    "train_acc": [
      0.38798972864312187,
      0.4073773833343486,
      0.40006981898607835,
      0.37668045864982264,
      0.35498502874761606,
      0.3435445552905454,
      0.33907426584633266,
      0.337075755942814,
      0.33666340219953234
    ],
    "valid_acc": [
      0.3668007995371101,
      0.38937983272842036,
      0.38823575824522644,
      0.3681158276787123,
      0.34826547788122664,
      0.3364104991846826,
      0.3318539266740308,
      0.329585503129767,
      0.329033191310294
    ],
    "epoch": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ],
    "timestamp": [
      1762406100.0595698,
      1762406324.9269853,
      1762406550.291549,
      1762406777.3660161,
      1762407004.847552,
      1762407230.6581483,
      1762407456.4346657,
      1762407682.952557,
      1762407909.2677863
    ]
  }
}