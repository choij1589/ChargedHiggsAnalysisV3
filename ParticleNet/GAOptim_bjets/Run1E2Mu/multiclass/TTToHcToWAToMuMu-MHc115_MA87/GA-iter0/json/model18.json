{
  "hyperparameters": {
    "signal": "MHc115_MA87",
    "channel": "Run1E2Mu",
    "iteration": 0,
    "model_idx": 18,
    "num_hidden": 128,
    "optimizer": "Adadelta",
    "initial_lr": 0.0002,
    "weight_decay": 1e-05,
    "scheduler": "ReduceLROnPlateau",
    "pilot_mode": false,
    "num_classes": 4,
    "model_type": "ParticleNet",
    "num_node_features": 9,
    "num_graph_features": 4,
    "dropout_p": 0.3,
    "batch_size": 1024,
    "train_folds": [
      0,
      1,
      2
    ],
    "valid_folds": [
      3
    ]
  },
  "training_summary": {
    "best_epoch": 1,
    "best_train_loss": 1.520975171419842,
    "best_valid_loss": 1.4892443372035966,
    "best_train_acc": 0.3121773435951357,
    "best_valid_acc": 0.33357109476842856,
    "total_epochs": 9
  },
  "epoch_history": {
    "train_loss": [
      1.5202930612424934,
      1.520975171419842,
      1.5325551357311533,
      1.5361498715076585,
      1.555740037471669,
      1.5702549607317577,
      1.5911899606070037,
      1.6132457093924804,
      1.6323847450344655
    ],
    "valid_loss": [
      1.490892926359646,
      1.4892443372035966,
      1.498003073364391,
      1.49977788788002,
      1.516840776366747,
      1.5294797012415797,
      1.5482976200648275,
      1.5685504627962497,
      1.586543850474009
    ],
    "train_acc": [
      0.31206099337234655,
      0.3121773435951357,
      0.31217518896138036,
      0.312176266278258,
      0.312176266278258,
      0.3121773435951357,
      0.31218165286264643,
      0.31219350334830087,
      0.3122182816364874
    ],
    "valid_acc": [
      0.3334781765064373,
      0.33357109476842856,
      0.33357382765848714,
      0.33357382765848714,
      0.33357382765848714,
      0.3335765605485457,
      0.3335765605485457,
      0.3335929578888971,
      0.333631218349717
    ],
    "epoch": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ],
    "timestamp": [
      1761018034.0055656,
      1761018487.8315074,
      1761018881.3162756,
      1761019343.0455143,
      1761019791.1929212,
      1761020251.0080795,
      1761020712.2654443,
      1761021113.7762716,
      1761021572.481811
    ]
  }
}