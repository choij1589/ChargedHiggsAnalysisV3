{
  "signal": "MHc130_MA90",
  "channel": "Run1E2Mu",
  "best_chromosome": {
    "chromosome": [
      128,
      "Adam",
      0.001,
      1e-05,
      "CyclicLR"
    ],
    "fitness": 0.8491744831693651,
    "model": "model0"
  },
  "configuration": {
    "description": "Genetic Algorithm hyperparameter optimization configuration for ParticleNet multi-class training",
    "version": "1.0",
    "ga_parameters": {
      "population_size": 12,
      "max_iterations": 3,
      "evolution_ratio": 0.7,
      "mutation_thresholds": [
        0.7,
        0.7,
        0.7,
        0.7,
        0.7
      ],
      "fitness_metric": "loss/valid",
      "overfitting_penalty_weight": 0.3,
      "comment": "mutation_thresholds: one per hyperparameter [nNodes, optimizer, initLR, weight_decay, scheduler]. overfitting_penalty_weight: \u03bb for fitness = valid_loss + \u03bb*(valid_loss - train_loss). Set to 0 to disable."
    },
    "hyperparameter_search_space": {
      "nNodes": {
        "type": "discrete",
        "values": [
          64,
          96,
          128
        ],
        "description": "Number of hidden nodes in convolution layers"
      },
      "optimizers": {
        "type": "discrete",
        "values": [
          "RMSprop",
          "Adam",
          "Adadelta"
        ],
        "description": "Optimizer algorithms"
      },
      "schedulers": {
        "type": "discrete",
        "values": [
          "ExponentialLR",
          "CyclicLR",
          "ReduceLROnPlateau"
        ],
        "description": "Learning rate schedulers"
      },
      "initLR": {
        "type": "log_uniform",
        "min": 0.0001,
        "max": 0.01,
        "samples": 100,
        "round_digits": 4,
        "description": "Initial learning rate (log-uniform sampled)"
      },
      "weight_decay": {
        "type": "log_uniform",
        "min": 1e-05,
        "max": 0.001,
        "samples": 100,
        "round_digits": 5,
        "description": "L2 regularization weight decay (log-uniform sampled)"
      }
    },
    "training_parameters": {
      "max_epochs": 81,
      "batch_size": 1024,
      "dropout_p": 0.4,
      "early_stopping_patience": 7,
      "loss_type": "weighted_ce",
      "train_folds": [
        0,
        1,
        2
      ],
      "valid_folds": [
        3
      ],
      "balance_weights": true,
      "max_events_per_fold_per_class": 50000,
      "comment": "train_folds [0,1,2] for training, valid_folds [3] for early stopping, test_folds [4] for overfitting detection. max_events_per_fold_per_class limits memory usage and speeds up training while preserving physics through weight rescaling."
    },
    "background_groups": {
      "nonprompt": [
        "TTLL_powheg"
      ],
      "diboson": [
        "WZTo3LNu_amcatnlo",
        "ZZTo4L_powheg"
      ],
      "ttX": [
        "TTZToLLNuNu",
        "tZq"
      ],
      "comment": "Physics-motivated background grouping for multi-class training"
    },
    "dataset_config": {
      "use_bjets": true,
      "signal_prefix": "TTToHcToWAToMuMu-",
      "background_prefix": "Skim_TriLep_",
      "comment": "use_bjets=true means use dataset_bjets with separate b-jet particles"
    },
    "model_config": {
      "default_model": "ParticleNet",
      "comment": "Model architecture - can be changed to ParticleNetV2, OptimizedParticleNet, etc."
    },
    "execution_config": {
      "parallel_training": true,
      "process_delay_seconds": 0.5,
      "comment": "Delay between launching parallel training processes to avoid resource conflicts"
    },
    "output_config": {
      "results_dir": "GAOptim_bjets",
      "ga_subdir_pattern": "GA-iter{iteration}",
      "json_subdir": "json",
      "models_subdir": "models",
      "model_name_pattern": "model{idx}",
      "population_summary_name": "model_info.csv",
      "comment": "Output directory structure for GA results - separate from regular training results"
    },
    "overfitting_detection": {
      "enabled": false,
      "p_value_threshold": 0.3,
      "test_folds": [
        4
      ],
      "comment": "Kolmogorov-Smirnov test on per-class score distributions. Model is overfitted if ANY class has p < threshold. Always regenerate filtered models to maintain population size."
    }
  }
}