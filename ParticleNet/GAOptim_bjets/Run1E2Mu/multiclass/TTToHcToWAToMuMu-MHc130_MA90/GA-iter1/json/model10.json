{
  "hyperparameters": {
    "signal": "MHc130_MA90",
    "channel": "Run1E2Mu",
    "iteration": 1,
    "model_idx": 10,
    "num_hidden": 128,
    "optimizer": "Adadelta",
    "initial_lr": 0.0015,
    "weight_decay": 0.00022,
    "scheduler": "ReduceLROnPlateau",
    "pilot_mode": false,
    "num_classes": 4,
    "model_type": "ParticleNet",
    "num_node_features": 9,
    "num_graph_features": 4,
    "dropout_p": 0.4,
    "batch_size": 1024,
    "train_folds": [
      0,
      1,
      2
    ],
    "valid_folds": [
      3
    ]
  },
  "training_summary": {
    "best_epoch": 1,
    "best_train_loss": 1.3069211171320174,
    "best_valid_loss": 1.3236121911210725,
    "best_train_acc": 0.34934409892411883,
    "best_valid_acc": 0.3438459077389769,
    "total_epochs": 9
  },
  "epoch_history": {
    "train_loss": [
      1.3167493691440808,
      1.3069211171320174,
      1.3247833884102513,
      1.3742517321228427,
      1.4589850149387606,
      1.5783537030838275,
      1.7113535344262463,
      1.7658685901553943,
      1.7979196646954017
    ],
    "valid_loss": [
      1.331481109097321,
      1.3236121911210725,
      1.3429119087822512,
      1.3939121797493863,
      1.4806053690582563,
      1.6011287259196127,
      1.7362039703026686,
      1.7933083174077267,
      1.8277808602533057
    ],
    "train_acc": [
      0.36543350485061726,
      0.34934409892411883,
      0.3102279891801745,
      0.3056092253248998,
      0.30522280298562104,
      0.3052126339766926,
      0.3053488986963331,
      0.3061319123838191,
      0.3081738493766398
    ],
    "valid_acc": [
      0.35920892038250524,
      0.3438459077389769,
      0.3023248924648363,
      0.29686814944721596,
      0.29642378926662794,
      0.2964000900569966,
      0.29651266130274556,
      0.2975791257361567,
      0.299747603417426
    ],
    "epoch": [
      0,
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ],
    "timestamp": [
      1762412168.3041282,
      1762412427.0637264,
      1762412687.156033,
      1762412947.5279171,
      1762413207.4259238,
      1762413468.2654064,
      1762413728.6567936,
      1762413988.6449203,
      1762414248.6170924
    ]
  }
}