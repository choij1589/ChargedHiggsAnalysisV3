{
  "description": "Genetic Algorithm hyperparameter optimization configuration for ParticleNet multi-class training",
  "version": "1.0",

  "ga_parameters": {
    "population_size": 12,
    "max_iterations": 4,
    "evolution_ratio": 0.7,
    "mutation_thresholds": [0.7, 0.7, 0.7, 0.7, 0.7],
    "fitness_metric": "loss/valid",
    "overfitting_penalty_weight": 0.3,
    "comment": "mutation_thresholds: one per hyperparameter [nNodes, optimizer, initLR, weight_decay, scheduler]. overfitting_penalty_weight: λ for fitness = valid_loss + λ*(valid_loss - train_loss). Set to 0 to disable."
  },

  "hyperparameter_search_space": {
    "nNodes": {
      "type": "discrete",
      "values": [96, 128, 192],
      "description": "Number of hidden nodes in convolution layers"
    },
    "optimizers": {
      "type": "discrete",
      "values": ["RMSprop", "Adam", "Adadelta"],
      "description": "Optimizer algorithms"
    },
    "schedulers": {
      "type": "discrete",
      "values": ["ExponentialLR", "CyclicLR", "ReduceLROnPlateau"],
      "description": "Learning rate schedulers"
    },
    "initLR": {
      "type": "log_uniform",
      "min": 1e-4,
      "max": 1e-2,
      "samples": 100,
      "round_digits": 4,
      "description": "Initial learning rate (log-uniform sampled)"
    },
    "weight_decay": {
      "type": "log_uniform",
      "min": 1e-5,
      "max": 1e-3,
      "samples": 100,
      "round_digits": 5,
      "description": "L2 regularization weight decay (log-uniform sampled)"
    }
  },

  "training_parameters": {
    "max_epochs": 100,
    "batch_size": 512,
    "dropout_p": 0.4,
    "early_stopping_patience": 7,
    "loss_type": "weighted_ce",
    "train_folds": [0, 1, 2],
    "valid_folds": [3],
    "balance_weights": true,
    "max_events_per_fold_per_class": 50000,
    "comment": "train_folds [0,1,2] for training, valid_folds [3] for early stopping, test_folds [4] for overfitting detection. max_events_per_fold_per_class limits memory usage and speeds up training while preserving physics through weight rescaling."
  },

  "background_groups": {
    "nonprompt": ["TTLL_powheg"],
    "diboson": ["WZTo3LNu_amcatnlo", "ZZTo4L_powheg"],
    "ttX": ["TTZToLLNuNu", "tZq"],
    "comment": "Physics-motivated background grouping for multi-class training"
  },

  "dataset_config": {
    "use_bjets": true,
    "signal_prefix": "TTToHcToWAToMuMu-",
    "background_prefix": "Skim_TriLep_",
    "comment": "use_bjets=true means use dataset_bjets with separate b-jet particles"
  },

  "model_config": {
    "default_model": "ParticleNet",
    "comment": "Model architecture - can be changed to ParticleNetV2, OptimizedParticleNet, etc."
  },

  "execution_config": {
    "parallel_training": true,
    "process_delay_seconds": 0.5,
    "comment": "Delay between launching parallel training processes to avoid resource conflicts"
  },

  "output_config": {
    "results_dir": "GAOptim_bjets",
    "ga_subdir_pattern": "GA-iter{iteration}",
    "json_subdir": "json",
    "models_subdir": "models",
    "model_name_pattern": "model{idx}",
    "population_summary_name": "model_info.csv",
    "comment": "Output directory structure for GA results - separate from regular training results"
  },

  "overfitting_detection": {
    "enabled": false,
    "p_value_threshold": 0.3,
    "test_folds": [4],
    "bin_merge_threshold": 1e-6,
    "bin_merge_max_iterations": 100,
    "comment": "Kolmogorov-Smirnov test on per-class score distributions. Model is overfitted if ANY class has p < threshold. bin_merge_threshold: merge bins with |content| < threshold or content < 0. bin_merge_max_iterations: maximum iterations for iterative bin merging."
  }
}
